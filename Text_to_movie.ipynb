{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tullsokk/text-to-movie/blob/main/Text_to_movie.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8ISDCrdIOuq"
      },
      "source": [
        "#Welcome to text-to-movie\n",
        "\n",
        "This notebook allows you to make an AI generated movie from a single prompt. Make sure to change Runtime to GPU. Free tier should give enough juice for a movie or two, after that you might have to pay for extra compute units. You need API tokes for:\n",
        "\n",
        "*   openAI\n",
        "*   elevenlabs\n",
        "\n",
        "Do not share your tokens! The final movie is saved under contents, and can be download, or you can mount your google drive and save it there"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVAT6mQmZtc7",
        "cellView": "form",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#@title Install dependencies\n",
        "#Installing imagemagick\n",
        "!apt install imagemagick\n",
        "\n",
        "!sudo apt-get install libportaudio2\n",
        "#Installing pip packages\n",
        "!pip install openai elevenlabs diffusers transformers accelerate pytorch-lightning git+https://github.com/huggingface/diffusers\n",
        "\n",
        "#You may have to remove or comment out the line   <policy domain=\"path\" rights=\"none\" pattern=\"@*\"/>\n",
        "# in the file /etc/ImageMagick-6/policy.xml for subtitles to work\n",
        "\n",
        "#removes a path policy from imagemagick that causes problems\n",
        "\n",
        "xml_string = \"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
        "<!DOCTYPE policymap [\n",
        "  <!ELEMENT policymap (policy)+>\n",
        "  <!ATTLIST policymap xmlns CDATA #FIXED ''>\n",
        "  <!ELEMENT policy EMPTY>\n",
        "  <!ATTLIST policy xmlns CDATA #FIXED '' domain NMTOKEN #REQUIRED\n",
        "    name NMTOKEN #IMPLIED pattern CDATA #IMPLIED rights NMTOKEN #IMPLIED\n",
        "    stealth NMTOKEN #IMPLIED value CDATA #IMPLIED>\n",
        "]>\n",
        "<!--\n",
        "  Configure ImageMagick policies.\n",
        "\n",
        "  Domains include system, delegate, coder, filter, path, or resource.\n",
        "\n",
        "  Rights include none, read, write, execute and all.  Use | to combine them,\n",
        "  for example: \"read | write\" to permit read from, or write to, a path.\n",
        "\n",
        "  Use a glob expression as a pattern.\n",
        "\n",
        "  Suppose we do not want users to process MPEG video images:\n",
        "\n",
        "    <policy domain=\"delegate\" rights=\"none\" pattern=\"mpeg:decode\" />\n",
        "\n",
        "  Here we do not want users reading images from HTTP:\n",
        "\n",
        "    <policy domain=\"coder\" rights=\"none\" pattern=\"HTTP\" />\n",
        "\n",
        "  The /repository file system is restricted to read only.  We use a glob\n",
        "  expression to match all paths that start with /repository:\n",
        "\n",
        "    <policy domain=\"path\" rights=\"read\" pattern=\"/repository/*\" />\n",
        "\n",
        "  Lets prevent users from executing any image filters:\n",
        "\n",
        "    <policy domain=\"filter\" rights=\"none\" pattern=\"*\" />\n",
        "\n",
        "  Any large image is cached to disk rather than memory:\n",
        "\n",
        "    <policy domain=\"resource\" name=\"area\" value=\"1GP\"/>\n",
        "\n",
        "  Define arguments for the memory, map, area, width, height and disk resources\n",
        "  with SI prefixes (.e.g 100MB).  In addition, resource policies are maximums\n",
        "  for each instance of ImageMagick (e.g. policy memory limit 1GB, -limit 2GB\n",
        "  exceeds policy maximum so memory limit is 1GB).\n",
        "\n",
        "  Rules are processed in order.  Here we want to restrict ImageMagick to only\n",
        "  read or write a small subset of proven web-safe image types:\n",
        "\n",
        "    <policy domain=\"delegate\" rights=\"none\" pattern=\"*\" />\n",
        "    <policy domain=\"filter\" rights=\"none\" pattern=\"*\" />\n",
        "    <policy domain=\"coder\" rights=\"none\" pattern=\"*\" />\n",
        "    <policy domain=\"coder\" rights=\"read|write\" pattern=\"{GIF,JPEG,PNG,WEBP}\" />\n",
        "-->\n",
        "<policymap>\n",
        "  <!-- <policy domain=\"system\" name=\"shred\" value=\"2\"/> -->\n",
        "  <!-- <policy domain=\"system\" name=\"precision\" value=\"6\"/> -->\n",
        "  <!-- <policy domain=\"system\" name=\"memory-map\" value=\"anonymous\"/> -->\n",
        "  <!-- <policy domain=\"system\" name=\"max-memory-request\" value=\"256MiB\"/> -->\n",
        "  <!-- <policy domain=\"resource\" name=\"temporary-path\" value=\"/tmp\"/> -->\n",
        "  <policy domain=\"resource\" name=\"memory\" value=\"256MiB\"/>\n",
        "  <policy domain=\"resource\" name=\"map\" value=\"512MiB\"/>\n",
        "  <policy domain=\"resource\" name=\"width\" value=\"16KP\"/>\n",
        "  <policy domain=\"resource\" name=\"height\" value=\"16KP\"/>\n",
        "  <!-- <policy domain=\"resource\" name=\"list-length\" value=\"128\"/> -->\n",
        "  <policy domain=\"resource\" name=\"area\" value=\"128MB\"/>\n",
        "  <policy domain=\"resource\" name=\"disk\" value=\"1GiB\"/>\n",
        "  <!-- <policy domain=\"resource\" name=\"file\" value=\"768\"/> -->\n",
        "  <!-- <policy domain=\"resource\" name=\"thread\" value=\"4\"/> -->\n",
        "  <!-- <policy domain=\"resource\" name=\"throttle\" value=\"0\"/> -->\n",
        "  <!-- <policy domain=\"resource\" name=\"time\" value=\"3600\"/> -->\n",
        "  <!-- <policy domain=\"coder\" rights=\"none\" pattern=\"MVG\" /> -->\n",
        "  <!-- <policy domain=\"module\" rights=\"none\" pattern=\"{PS,PDF,XPS}\" /> -->\n",
        "  <!-- <policy domain=\"delegate\" rights=\"none\" pattern=\"HTTPS\" /> -->\n",
        "  <!-- <policy domain=\"path\" rights=\"none\" pattern=\"@*\" /> -->\n",
        "  <!-- <policy domain=\"cache\" name=\"memory-map\" value=\"anonymous\"/> -->\n",
        "  <!-- <policy domain=\"cache\" name=\"synchronize\" value=\"True\"/> -->\n",
        "  <!-- <policy domain=\"cache\" name=\"shared-secret\" value=\"passphrase\" stealth=\"true\"/> -->\n",
        "  <!-- <policy domain=\"system\" name=\"pixel-cache-memory\" value=\"anonymous\"/> -->\n",
        "  <!-- <policy domain=\"system\" name=\"shred\" value=\"2\"/> -->\n",
        "  <!-- <policy domain=\"system\" name=\"precision\" value=\"6\"/> -->\n",
        "  <!-- not needed due to the need to use explicitly by mvg: -->\n",
        "  <!-- <policy domain=\"delegate\" rights=\"none\" pattern=\"MVG\" /> -->\n",
        "  <!-- use curl -->\n",
        "  <policy domain=\"delegate\" rights=\"none\" pattern=\"URL\" />\n",
        "  <policy domain=\"delegate\" rights=\"none\" pattern=\"HTTPS\" />\n",
        "  <policy domain=\"delegate\" rights=\"none\" pattern=\"HTTP\" />\n",
        "  <!-- in order to avoid to get image with password text -->\n",
        "  <!-- disable ghostscript format types -->\n",
        "  <policy domain=\"coder\" rights=\"none\" pattern=\"PS\" />\n",
        "  <policy domain=\"coder\" rights=\"none\" pattern=\"PS2\" />\n",
        "  <policy domain=\"coder\" rights=\"none\" pattern=\"PS3\" />\n",
        "  <policy domain=\"coder\" rights=\"none\" pattern=\"EPS\" />\n",
        "  <policy domain=\"coder\" rights=\"none\" pattern=\"PDF\" />\n",
        "  <policy domain=\"coder\" rights=\"none\" pattern=\"XPS\" />\n",
        "</policymap>\"\"\"\n",
        "\n",
        "with open(\"/etc/ImageMagick-6/policy.xml\", \"w\") as f:\n",
        "    f.write(xml_string)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ByOfd3Z8eX_N",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Import packages\n",
        "import cv2, openai, os, json, moviepy, string, torch, urllib.request, soundfile as sf, wave, numpy as np, nltk, math, scipy, shutil, pathlib, transformers, gc\n",
        "\n",
        "from moviepy.editor import *\n",
        "from moviepy.video.io.VideoFileClip import VideoFileClip\n",
        "from moviepy.audio.io.AudioFileClip import AudioFileClip\n",
        "from moviepy.video.tools.subtitles import SubtitlesClip\n",
        "from elevenlabs import voices, generate, set_api_key\n",
        "from diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\n",
        "from diffusers.utils import export_to_video\n",
        "from diffusers import MusicLDMPipeline\n",
        "from diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\n",
        "from huggingface_hub import snapshot_download\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DfyJEUrx35Zj",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Insert your openai and elevelabs api tokens.\n",
        "openai_key = ''#@param {type:\"string\"}\n",
        "elevenlabstoken = \"\"#@param {type:\"string\"}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mpbVOodan6on",
        "cellView": "form",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#@title set up pipelines. Chose modelscope or zeroscope (better resolution)\n",
        "model = \"Zeroscope\" #@param [\"Modelscope\", \"Zeroscope\"]\n",
        "\n",
        "if model == 'Zeroscope':\n",
        "  pipe = DiffusionPipeline.from_pretrained(\"cerspense/zeroscope_v2_576w\", torch_dtype=torch.float16)\n",
        "elif model == 'Modelscope':\n",
        "# load pipeline for text to video generation\n",
        "  pipe = DiffusionPipeline.from_pretrained(\"damo-vilab/text-to-video-ms-1.7b\", torch_dtype=torch.float16, variant=\"fp16\")\n",
        "\n",
        "pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n",
        "\n",
        "# optimize for GPU memory\n",
        "pipe.enable_model_cpu_offload()\n",
        "pipe.enable_vae_slicing()\n",
        "\n",
        "#loads pipeline for text to music\n",
        "repo_id = \"cvssp/musicldm\"\n",
        "music_pipe = MusicLDMPipeline.from_pretrained(repo_id, torch_dtype=torch.float16)\n",
        "music_pipe = music_pipe.to(\"cuda\")\n",
        "\n",
        "#helper for parsing text\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Aif4EH9CJCp",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Define functions\n",
        "\n",
        "#returns list of cast\n",
        "def persons(movie_script):\n",
        "    data = json.loads(movie_script)\n",
        "    persons = []\n",
        "    for item in data:\n",
        "      # Iterate over each item in the dialogue list\n",
        "      for dialogue_item in item[\"dialogue\"]:\n",
        "        # Print the speaker and text for each item\n",
        "        key = dialogue_item['person']\n",
        "        persons.append(key)\n",
        "    persons = list(dict.fromkeys(persons))\n",
        "\n",
        "\n",
        "    return persons\n",
        "\n",
        "# Sends the elevenlabs list of voice actors and casts the caracters. It tends to do a good job with both age, gender and description,\n",
        "def casting(persons):\n",
        "  response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-4\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": 'Here is a list of descriptions of characters. ' + str(all_voices)  + ' I will give you a list of characters names. I want you to assign a suitable voice actor to each of the characters and return only text in a json format: [{\"character\":\"Han Solo\", \"actor\": \"Clyde\"}]'},\n",
        "                {\"role\": \"user\", \"content\": str(persons)},\n",
        "            ]\n",
        "        )\n",
        "  message = response.choices[0][\"message\"][\"content\"]\n",
        "  return message\n",
        "\n",
        "#Function for creating scene from prompt. default is 8 frames per second, 40 frames = five seconds\n",
        "#def createScene(prompt, duration):\n",
        "#  video_frames = pipe(prompt, num_inference_steps=25, num_frames=duration).frames\n",
        "#  # convert to video\n",
        "#  video_path = export_to_video(video_frames)\n",
        "#  return video_path\n",
        "\n",
        "#def createScene(prompt, duration):\n",
        "#  video_frames = pipe(prompt, num_inference_steps=25, height=320, width=576, num_frames=duration).frames\n",
        "#  # convert to video\n",
        "#  video_path = export_to_video(video_frames)\n",
        "#  torch.cuda.empty_cache()\n",
        "#\n",
        "#  # Collect garbage\n",
        "#  gc.collect()\n",
        "#  return video_path\n",
        "#\n",
        "#  return video_path\n",
        "\n",
        "def createScene(prompt, duration):\n",
        "    while True:\n",
        "        print('Trying to create clip with duration: ' + str(duration))\n",
        "        try:\n",
        "            video_frames = pipe(prompt, num_inference_steps=25, height=320, width=576, num_frames=duration).frames\n",
        "            break\n",
        "        except:\n",
        "            print('failed at duration : ' + str(duration) + '. reducing duration by 10 percent.')\n",
        "            duration = int(duration * 0.9)\n",
        "    # convert to video\n",
        "    video_path = export_to_video(video_frames)\n",
        "    # Collect garbage\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    return video_path\n",
        "\n",
        "\n",
        "# Generate an audio object from the voice name and the text\n",
        "def generateSpeech(line, voice, output):\n",
        "  audio = generate(text=line,voice=voice)\n",
        "  with open(output + \".wav\", \"wb\") as f:\n",
        "    f.write(audio)\n",
        "  return\n",
        "\n",
        "# The model i use can sometimes return just a noisy pattern.\n",
        "# The following tries to remedy this by checking if the returned video is just noise\n",
        "# If too much noise, it retries up to n times\n",
        "def is_video_noisy(video_path,threshold):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    lap = 0\n",
        "    iter = 1\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "\n",
        "        if not ret:\n",
        "            break\n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "        laplacian = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
        "        iter +=1\n",
        "        lap += laplacian\n",
        "\n",
        "    avg_laplacian = lap/iter\n",
        "    print(\"avg laplacian:\" + str(avg_laplacian))\n",
        "    if avg_laplacian > threshold:\n",
        "      return True, avg_laplacian\n",
        "    else:\n",
        "      return False, avg_laplacian\n",
        "\n",
        "def test_for_noise(line, max_tries, duration, noise_threshold):\n",
        "  min_laplacian = float('inf')\n",
        "  min_video = None\n",
        "  for i in range(max_tries):\n",
        "    print(\"try number \" + str(i))\n",
        "    new_scene = createScene(line,duration)\n",
        "    video = VideoFileClip(new_scene)\n",
        "    filename = clipname + \"_tmp\" + str(i) + \".mp4\"\n",
        "    video.write_videofile(filename, fps=video.fps)\n",
        "    bool_, avg_lap = is_video_noisy(filename, noise_threshold)\n",
        "    if avg_lap < min_laplacian:\n",
        "      min_laplacian = avg_lap\n",
        "      min_video = video\n",
        "    if not bool_:\n",
        "      print(\"video passed noise threshold!\")\n",
        "      return min_video\n",
        "    if bool_:\n",
        "      print(\"video is too noisy, trying again\")\n",
        "      if i == max_tries - 1:\n",
        "        print(\"Was not able to create a clip in \" + str(max_tries) + \" attempts\")\n",
        "\n",
        "  return min_video\n",
        "\n",
        "# Funcion for generating music\n",
        "def generateMusic(music,length,scene):\n",
        "  audio = music_pipe(music, num_inference_steps=200, audio_length_in_s=length).audios[0]\n",
        "  scipy.io.wavfile.write(\"Music_\" + str(scene) + \".wav\", rate=16000, data=audio)\n",
        "  return\n",
        "\n",
        "def generateSilence(silence_duration):\n",
        "  # Creates a silent wav file for padding\n",
        "\n",
        "  filename = \"silence.wav\"\n",
        "  nchannels = 1\n",
        "  sampwidth = 2\n",
        "  framerate = 44100\n",
        "  nframes = int(framerate * silence_duration)\n",
        "\n",
        "  with wave.open(filename, \"w\") as f:\n",
        "      f.setnchannels(nchannels)\n",
        "      f.setsampwidth(sampwidth)\n",
        "      f.setframerate(framerate)\n",
        "      f.setnframes(nframes)\n",
        "\n",
        "      # Remove audio data\n",
        "      data = np.zeros(nframes, dtype=np.int16)\n",
        "      f.writeframes(data.tobytes())\n",
        "  silence = AudioFileClip(\"silence.wav\")\n",
        "  return silence\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2OV9zlWHjV5T",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Create movie\n",
        "#@markdown Enter a desired movie title.\n",
        "movietitle =  \"Game of Thrones - The Kraken returns\" #@param {type:\"string\"}\n",
        "video_model = 'zeroscope'#@param {type:\"string\"}\n",
        "#@markdown To get good results, use a description of familiar casts for the movie. A generic John doe will be rendered differently each scene, but Luke Skywalker will be more consistent.\n",
        "plot = \"A familiar cast of characters from the game of thrones universe reconcile their differences to assemble a great army and defeat the Kraken\"#@param {type:\"string\"}\n",
        "\n",
        "#@markdown Generating a 5 scene movie can take about half an hour on a T4 GPU, or 6 minutes on an A100, and give a 1-2 minute movie depending on the plot. I have not tried much longer movies\n",
        "\n",
        "scene_count = 5#@param {type:\"number\"}\n",
        "prompt = \"The title of the movie is\" + movietitle + \"The plot of the movie: \" + plot\n",
        "systemprompt = 'I want you to write a movie script for a short movie. You should provide a funny, comedic script that is original and surprising. I want you to answer in a nested json format. Always use the whole name, e.g. Jon Snow, never just Jon, Snow or he, to describe the scene. The movie should have at least ' + str(scene_count) + ' scenes. Describe in detail what happens in each scene. Each dialogue item should only contain one sentance. If a character says several sentences in a row, each line should be its own item. Desired output is this json format:[{\"scene\": \"description of the scene\",\"music\": \"description of the music\",\"dialogue\": [{\"person\": \"Person A\",\"line\":\"The first line person A says\"},{\"person\":\"Person A\",\"line\":\"The second line Person A says\"}, {\"person\": ...}]},{\"scene\": ...}]'\n",
        "set_api_key(elevenlabstoken)\n",
        "\n",
        "# Adjust the systemprompt text if you wish to experiment with the format.\n",
        "# Sometimes, the response is not properly formatted.\n",
        "# I implemented a loop to send back the answer if it is not proper json, which run until valid, or timeout after 10 tries.\n",
        "# This version uses GPT4. GTP3.5-Turbo is faster and might provide sufficient results.\n",
        "# This was originally developed using the davinci-002 model (GPT-3).\n",
        "# One issue is that a scene descripiton might reference he or she, not the actual character name.\n",
        "# This causes issues for generating video. I have tried instructing GPT to not do this, but it still does.\n",
        "# This can be fixed by furter system prompting or implementing a check for he/she/they, that returns the prompt and ask GPT to replace with the names, but this is not yet implemented\n",
        "\n",
        "def generate_movie_script(prompt, counter=0):\n",
        "    print(\"counter: \" + str(counter))\n",
        "    if counter == 10:\n",
        "        print(\"Sorry, I could not generate a valid response after 10 attempts.\")\n",
        "        return \"Sorry, I could not generate a valid response after 10 attempts.\"\n",
        "    openai.api_key = openai_key\n",
        "    response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-4\",\n",
        "            messages=[\n",
        "                {\"role\": \"user\", \"content\": prompt},\n",
        "                {\"role\": \"system\", \"content\": systemprompt},\n",
        "\n",
        "            ]\n",
        "        )\n",
        "    message = response.choices[0][\"message\"][\"content\"]\n",
        "    print(message)\n",
        "    if not message:\n",
        "        return generate_movie_script(prompt, counter + 1)\n",
        "    try:\n",
        "        json_obj = json.loads(message)\n",
        "    except ValueError:\n",
        "        print('reply not valid json, asking gpt to try again')\n",
        "        prompt += \"\\nThe reply you gave was not valid JSON. Can you please format the response as I have instructed you?\"\n",
        "        return generate_movie_script(prompt, counter + 1)\n",
        "    return message\n",
        "\n",
        "movie_script = generate_movie_script(prompt)\n",
        "\n",
        "#get all persons from script\n",
        "persons = persons(movie_script)\n",
        "print(persons)\n",
        "\n",
        "# Get a list of all premade voices from elevenlabs - along with descriptions of the voices\n",
        "all_voices = voices()\n",
        "\n",
        "# Extracting the relevant information to pass to GTP-4 (using all the info expends unnecessarily many tokens)\n",
        "voices_list = []\n",
        "for voice in all_voices:\n",
        "  voice_dict = {}\n",
        "  voice_dict[\"name\"] = str(voice.name)\n",
        "  voice_dict[\"accent\"] = str(voice.labels[\"accent\"])\n",
        "  voice_dict[\"description\"] = str(voice.labels[\"description\"]) if \"description\" in voice.labels else \"\"\n",
        "  voice_dict[\"age\"] = str(voice.labels[\"age\"])\n",
        "  voice_dict[\"gender\"] = str(voice.labels[\"gender\"])\n",
        "  voice_dict[\"use_case\"] = str(voice.labels[\"use case\"]) if \"use case\" in voice.labels else \"\"\n",
        "  voices_list.append(voice_dict)\n",
        "voices_json = json.dumps(voices_list)\n",
        "#print(voices_json)\n",
        "\n",
        "# Sends the elevenlabs list of voice actors and casts the caracters. It tends to do a good job with both age, gender and description,\n",
        "cast = casting(persons)\n",
        "\n",
        "print(cast)\n",
        "\n",
        "#reformats the response for later use\n",
        "json_object = json.loads(cast)\n",
        "voicematch = {item[\"character\"]:item[\"actor\"] for item in json_object}\n",
        "print(voicematch)\n",
        "\n",
        "#the shit\n",
        "\n",
        "#@markdown Noise threshold determines the threshold for re-trying video generation.\n",
        "noise_threshold = 800#@param {type:\"number\"}\n",
        "\n",
        "#@markdown Max_retries determies the maximum number of retries before moving on to the next scene\n",
        "max_retries = 5#@param {type:\"number\"}\n",
        "generator = lambda txt: TextClip(txt, font='Georgia-Regular', size = (1800,100), fontsize=24, color='white', method='caption')\n",
        "data=json.loads(movie_script)\n",
        "scenenumber = 0\n",
        "clips = {}\n",
        "scene_cuts = {}\n",
        "scene_lengths = {}\n",
        "dialouges = []\n",
        "soundfiles = []\n",
        "musicfiles = []\n",
        "rendered_scenes = []\n",
        "movielength = 0\n",
        "for item in data:\n",
        "  for key, value in item.items():\n",
        "    substring = \"scene\"\n",
        "    if substring.lower() in key.lower():\n",
        "      scenenumber += 1\n",
        "      scene_duration = 0\n",
        "      cuts = 0\n",
        "      #print(value)\n",
        "      lines = nltk.sent_tokenize(value)\n",
        "      line_count = len(lines)\n",
        "      print(\"scene\" + str(scenenumber) + \" has \" + str(line_count) + \" clips\")\n",
        "      print(\"scene is \" + str(line_count*5) + \" seconds long\")\n",
        "      for line in lines:\n",
        "        if len(line) > 0:\n",
        "          cuts += 1\n",
        "          clipname = f\"scene_{scenenumber}_\" + f\"cut_{cuts}\"\n",
        "          prompt = line\n",
        "          clips.update({clipname:prompt})\n",
        "          print(\"creating video: \" + line)\n",
        "          #40 frames, 8 fps, = 5 sek\n",
        "          duration = 40\n",
        "\n",
        "          video=test_for_noise(line,max_retries, duration, noise_threshold)\n",
        "          video = video.resize(height=1080)\n",
        "          start_time = 0\n",
        "          end_time = duration/8\n",
        "          sub = [((start_time, end_time),line)]\n",
        "          captions = SubtitlesClip(sub, generator)\n",
        "          final = CompositeVideoClip([video, captions.set_pos(('center','top'))])\n",
        "          final.write_videofile(clipname + \".mp4\", fps=video.fps)\n",
        "          rendered_scenes.append(final)\n",
        "          movielength += duration\n",
        "          scene_duration += duration\n",
        "\n",
        "      for dialogue_item in item[\"dialogue\"]:\n",
        "          # Print the speaker and text for each item\n",
        "          cuts += 1\n",
        "          key = dialogue_item['person']\n",
        "          value = dialogue_item['line']\n",
        "          line = key + ' : ' + value\n",
        "          voice = voicematch[key]\n",
        "          print(\"person \" +  key + \" will use the voice \" + voice + \" to say \" + value)\n",
        "          clipname = f\"scene_{scenenumber}_\" + f\"cut_{cuts}\"\n",
        "\n",
        "          generateSpeech(value, voice, clipname)\n",
        "          soundfile=AudioFileClip(clipname + \".wav\")\n",
        "          soundfile.write_audiofile(clipname + \"_s.wav\")\n",
        "          f = sf.SoundFile(clipname + \"_s.wav\")\n",
        "          cliplength = (len(f)/f.samplerate)\n",
        "\n",
        "          soundclips = []\n",
        "          if cliplength <= 40:\n",
        "            silence = generateSilence(0.1)\n",
        "          elif cliplength > 40:\n",
        "            silence = generateSilence(1)\n",
        "\n",
        "          soundclips.append(silence)\n",
        "          soundclips.append(soundfile)\n",
        "          soundclips.append(silence)\n",
        "          combined = concatenate_audioclips(soundclips)\n",
        "          combined.write_audiofile(clipname + \"_s.wav\")\n",
        "\n",
        "          f = sf.SoundFile(clipname + \"_s.wav\")\n",
        "          cliplength = (len(f)/f.samplerate)\n",
        "\n",
        "          duration = math.ceil(cliplength*8)\n",
        "          print (\"dialouge is \" + str(cliplength) + \" seconds and \" + str(duration)  + \" frames long\")\n",
        "          clipname = f\"scene_{scenenumber}_\" + f\"cut_{cuts}\"\n",
        "          prompt = f\"midshot of {key} talking\"\n",
        "          clips.update({clipname:prompt})\n",
        "          print(\"creating video \"+ clipname + \". with prompt: \" + prompt)\n",
        "          video=test_for_noise(prompt,max_retries, duration, noise_threshold)\n",
        "\n",
        "          dialogue = AudioFileClip(clipname + \".wav\")\n",
        "          video = video.set_audio(dialogue)\n",
        "          video = video.resize(height=1080)\n",
        "          start_time = 0\n",
        "          end_time = cliplength\n",
        "          sub = [((start_time, end_time),line)]\n",
        "\n",
        "          subtitles = SubtitlesClip(sub, generator)\n",
        "\n",
        "          final = CompositeVideoClip([video, subtitles.set_pos(('center','bottom'))])\n",
        "          final.write_videofile(clipname + \".mp4\", fps=video.fps)\n",
        "\n",
        "          rendered_scenes.append(final)\n",
        "          movielength += duration\n",
        "          scene_duration += duration\n",
        "    substring_ = \"music\"\n",
        "    if substring_.lower() in key.lower():\n",
        "      length = scene_duration/8\n",
        "      print(\"Scene \" + str(scenenumber) + \" is \" + str(scene_duration) + \" frames long/\" + str(length) + \" seconds long\" )\n",
        "      print(\"The music prompt:\" + value)\n",
        "      generateMusic(value,length, scenenumber)\n",
        "      musicfile = \"Music_\" + str(scenenumber)\n",
        "      musicfiles.append(musicfile)\n",
        "\n",
        "\n",
        "  scene_lengths.update({scenenumber:scene_duration})\n",
        "\n",
        "\n",
        "music_append = []\n",
        "for music in musicfiles:\n",
        "    musicfile = AudioFileClip(music + \".wav\")\n",
        "    music_append.append(musicfile)\n",
        "\n",
        "combined = concatenate_audioclips(music_append)\n",
        "\n",
        "combined.write_audiofile(\"soundtrack.wav\")\n",
        "\n",
        "final = concatenate_videoclips(rendered_scenes)\n",
        "final.write_videofile( movietitle + \"_.mp4\")\n",
        "video = VideoFileClip(movietitle + \"_.mp4\")\n",
        "soundtrack = AudioFileClip(\"soundtrack.wav\").volumex(0.1)\n",
        "final_audio = CompositeAudioClip([video.audio,soundtrack])\n",
        "final_video = video.set_audio(final_audio)\n",
        "final_video.write_videofile(movietitle + \".mp4\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#mount drive to save file straight to google drive, or just download it\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "HXWCZwlv5Y96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Run this if you dont have the folder already\n",
        "!mkdir -p \"/content/drive/My Drive/Text-to-movie\""
      ],
      "metadata": {
        "id": "hDkPPzhr5ez5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title copy movie to text-to-movie folder in google drive\n",
        "shutil.copy('/content/' + movietitle + \".mp4\",'/content/drive/My Drive/Text-to-movie/' + movietitle + \".mp4\")"
      ],
      "metadata": {
        "id": "7w3bMlua6DCr"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "mount_file_id": "1z15g0J5imhyU5qtAubThYa4nqX3kfjUE",
      "authorship_tag": "ABX9TyMEkQJIjXCduTed30SToQgM",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}